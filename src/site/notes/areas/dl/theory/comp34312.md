---
{"dg-publish":true,"permalink":"/areas/dl/theory/comp34312/","tags":["comp34312"]}
---

###  Bias Variance Decomposition
**Noise**$$\mathbb{E}_{y|x}[y - \mathbb{E}_{y|x}[y]]^2$$
**Bias**$$[\mathbb{E}_D[f(x)] - \mathbb{E}_{y|x}[y]]^2$$**Variance**$$\mathbb{E}_D[f(x) - \mathbb{E}_D(f(x))]^2$$
* *Double Descent:* neural nets typically exhibit low bias and high variance. if over-trained the variance + bias increases over time however if in the over-parameterised regime ($p>>n$) both bias and variance start decreasing again
### Bias Variance Decomposition for Ensembles

**Ambiguity of the Ensemble**$$\frac{1}{m} \sum^m f_i(x) - \bar{f}(x)$$
* loss of the ensemble is guaranteed to be less than or equal to the average loss i.e. ensemble will at least be better than the average
* overall ensemble variance is reduced by a factor of $1/m$ relative to the average

* expected loss = avg bias + avg variance - expected ambiguity  q(diversity)
	* the improvement in performance is entirely determined by diversity

---

### Empirical Risk Minimisation
* $f_{\text{erm}}$ empirical risk minimiser 
* $f^*$ best model in a given family (lowest population risk in family)
* $y^*$ bayes model

* **Excess Risk:** $R(f) - R(f^*)$
* **Estimation/Approximation Decomposition:** $R(f_{\text{erm}}) - R(y^*) = R(f_{\text{erm}}) - R(f^*) + R(f^*) - R(y^*)$
	* **Estimation Error:** $R(f_{\text{erm}}) - R(f^*)$ (random depends on sample size)
	* **Approximation Error:** $R(f^*) - R(f)$ (constant, depends on choice of model)
* **Optimisation/Estimation/Approximation Decomposition**$R(f) - R(y^*) = R(f) - R(f_{\text{erm}}) + R(f_{\text{erm}}) - R(f^*) + R(f^*) - R(y^*)$
	* **Optimisation Error:** $R(f) - R(f_{\text{erm}})$


---
### Linear Regression

$$\beta = X^\top (X X^\top)^{-1}y$$

---
### GD

$$t \geq \frac{\log (\frac{|x_0|}{\epsilon})}{\log \frac{1}{k}}$$

---
### GD in Over-parameterised Linear Regression

> [! note] because of over-parameterisation GD converges to a global minima nearest to the origin i.e. over-parameterisation induces an effect called "implicit bias" or "algorithmic regularisation"

* If $X \in \mathbb{R}^{n \times d}$ is of rank $n$ and $n < d$ then $X X^\top \in \mathbb{R}^{n \times n}$ is invertible

Objective Function$$\large \min_{\beta \in \mathbb{R}^{d}} \frac{1}{2} \Vert y - X \beta \Vert_2^2$$Loss function$$\hat{R}(\beta) = \frac{1}{2} \cdot \Vert y - X \beta\Vert_2^2 $$
* Assumptions:
	* over-parameterised i.e. $n < d$
	* $X$ is full rank

$$X^\dagger := X^\top (X X^\top)^{-1} \in \mathbb{R}^{d \times n} \hspace{2em} X X^\dagger = I$$

**NOTE:**
* $\Vert y - X \beta \Vert_2 = 0$ iff $\beta = X^{\dagger}y + \xi$ s.t. $\xi x_i = 0 \forall i$ multiplicity of global minima, there are uncountably many infinite global minima which can be described as $X^\dagger y + \text{Orth(Rows(x))}$
  
* $X^{\dagger}y = \arg \min \Vert \beta \Vert_2$ s.t. $\hat{R}(\beta) = 0$ i.e. 2-norm of all global minimisers of $\hat{R}$ is bigger than that of $X^\dagger y$

* Full-rank over-parameterised Linear Regression can be set up ($\beta_0 = 0$) such that GD find the minimum 2-norm interpolant ($X^\dagger y$)